{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x246dfd25790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset we use is called 'tiny shakespeare', containing all works of Shakespeare in a 1mb txt file\n",
    "\n",
    "# read the file\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters:\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# all the unique characters that occur in this text\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
      "[46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43]\n",
      "hello there\n"
     ]
    }
   ],
   "source": [
    "# create mappings of characters to and from integers\n",
    "\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "print(itos)\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]  # encoder: takes a string, returns a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l])  # decoder: takes a list of integers, returns a string\n",
    "\n",
    "print(encode('hello there'))\n",
    "print(decode([46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire dataset and store it in a tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])  # this is what GPT sees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's split this data into train and validation sets\n",
    "\n",
    "n = int(0.9*len(data))  # 90% train, 10% test\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8  # context length\n",
    "train_data[:block_size+1]  # we do plus one to get 9 characters, because in a chunk of 9 characters, there are 8 possible examples\n",
    "\n",
    "# for example:\n",
    "# given 18 ---> 47 comes next\n",
    "# given 18, 47 ---> 56 comes next\n",
    "# ...and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]), output is 47\n",
      "when input is tensor([18, 47]), output is 56\n",
      "when input is tensor([18, 47, 56]), output is 57\n",
      "when input is tensor([18, 47, 56, 57]), output is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]), output is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), output is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), output is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), output is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]  # inputs\n",
    "y = train_data[1:block_size+1]  # target outputs\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when input is {context}, output is {target}')\n",
    "\n",
    "# this is done to make the transformer used to seeing contexts all the way from a context length of 1 to a context length of 8\n",
    "# so it can start generating even when it is given just one character as an input\n",
    "# and it can generate even when it is given upto 8 characters as input\n",
    "\n",
    "# This is like our makemore version, but the difference is, in makemore, we had a fixed input size. \n",
    "# All names were 8 characters long. If not, we used to pad the . character to make them 8 long \n",
    "# here, instead we append the characters gradually so that the model could learn the combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "outputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "-----\n",
      "when input is [24], the target is: 43\n",
      "L, ---> e\n",
      "\n",
      "when input is [24, 43], the target is: 58\n",
      "Le, ---> t\n",
      "\n",
      "when input is [24, 43, 58], the target is: 5\n",
      "Let, ---> '\n",
      "\n",
      "when input is [24, 43, 58, 5], the target is: 57\n",
      "Let', ---> s\n",
      "\n",
      "when input is [24, 43, 58, 5, 57], the target is: 1\n",
      "Let's, --->  \n",
      "\n",
      "when input is [24, 43, 58, 5, 57, 1], the target is: 46\n",
      "Let's , ---> h\n",
      "\n",
      "when input is [24, 43, 58, 5, 57, 1, 46], the target is: 43\n",
      "Let's h, ---> e\n",
      "\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43], the target is: 39\n",
      "Let's he, ---> a\n",
      "\n",
      "\n",
      "when input is [44], the target is: 53\n",
      "f, ---> o\n",
      "\n",
      "when input is [44, 53], the target is: 56\n",
      "fo, ---> r\n",
      "\n",
      "when input is [44, 53, 56], the target is: 1\n",
      "for, --->  \n",
      "\n",
      "when input is [44, 53, 56, 1], the target is: 58\n",
      "for , ---> t\n",
      "\n",
      "when input is [44, 53, 56, 1, 58], the target is: 46\n",
      "for t, ---> h\n",
      "\n",
      "when input is [44, 53, 56, 1, 58, 46], the target is: 39\n",
      "for th, ---> a\n",
      "\n",
      "when input is [44, 53, 56, 1, 58, 46, 39], the target is: 58\n",
      "for tha, ---> t\n",
      "\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58], the target is: 1\n",
      "for that, --->  \n",
      "\n",
      "\n",
      "when input is [52], the target is: 58\n",
      "n, ---> t\n",
      "\n",
      "when input is [52, 58], the target is: 1\n",
      "nt, --->  \n",
      "\n",
      "when input is [52, 58, 1], the target is: 58\n",
      "nt , ---> t\n",
      "\n",
      "when input is [52, 58, 1, 58], the target is: 46\n",
      "nt t, ---> h\n",
      "\n",
      "when input is [52, 58, 1, 58, 46], the target is: 39\n",
      "nt th, ---> a\n",
      "\n",
      "when input is [52, 58, 1, 58, 46, 39], the target is: 58\n",
      "nt tha, ---> t\n",
      "\n",
      "when input is [52, 58, 1, 58, 46, 39, 58], the target is: 1\n",
      "nt that, --->  \n",
      "\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1], the target is: 46\n",
      "nt that , ---> h\n",
      "\n",
      "\n",
      "when input is [25], the target is: 17\n",
      "M, ---> E\n",
      "\n",
      "when input is [25, 17], the target is: 27\n",
      "ME, ---> O\n",
      "\n",
      "when input is [25, 17, 27], the target is: 10\n",
      "MEO, ---> :\n",
      "\n",
      "when input is [25, 17, 27, 10], the target is: 0\n",
      "MEO:, ---> \n",
      "\n",
      "\n",
      "when input is [25, 17, 27, 10, 0], the target is: 21\n",
      "MEO:\n",
      ", ---> I\n",
      "\n",
      "when input is [25, 17, 27, 10, 0, 21], the target is: 1\n",
      "MEO:\n",
      "I, --->  \n",
      "\n",
      "when input is [25, 17, 27, 10, 0, 21, 1], the target is: 54\n",
      "MEO:\n",
      "I , ---> p\n",
      "\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54], the target is: 39\n",
      "MEO:\n",
      "I p, ---> a\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4  # how many independent sequences will we process in parallel\n",
    "block_size = 8  # maximum context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  # randomly selects batch_size number of starting indices, that's why we do len(data) - block_size, so we don't choose the last few indices to begin with\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print('outputs:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('-----')\n",
    "\n",
    "for b in range(batch_size):  # batch dimension\n",
    "    for t in range(block_size):  # block dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]  # imp to not get confused: we index into yb here, not xb\n",
    "        print(f'when input is {context.tolist()}, the target is: {target}')\n",
    "        print(f'{decode(context.tolist())}, ---> {decode([target.tolist()])}')  # just for clarity\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65]) tensor(4.7341, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Pzb.nAIijJooLkmUc fAe?fyKmo:IOuc.rkGKqRYtKJcyJSldKsH?;Fi?X&aQEbtQpUkw'u b:HBVVTVoEy3X.:sgdfg;nOJk:.c\n"
     ]
    }
   ],
   "source": [
    "# bigram language model\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # embedding table of containing vocab_size dimension vectors\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "        logits = self.token_embedding_table(idx)  # (B, T, C)  the third dimension is the embeddings\n",
    "\n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # cross_entropy expects arguments to be of shape (N, C, d1, d2, ...) where N is batch size, C is number of classes\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)  # 4*8=32 samples with 65 classes each\n",
    "            targets = targets.view(-1)  # cross_entropy expects target of shape (N, d1, d2, ...), i.e. 4*8 = 32\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # (B, C)  # pluck out the last element in the time dimension across all batches\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape, loss)  # loss at initialization given a prob of 1/65 should be ~4.17\n",
    "\n",
    "# idx = torch.zeros((1, 1), dtype=torch.long)  # batch size: 1, time size: 1 containing a zero\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4595069885253906\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    # sample a batch of data \n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I mee hon he ithauleen's?-\n",
      "a gheo ing, pisourthest t ameer:\n",
      "myen have,\n",
      "\n",
      "Malset ubedy acepy ng t INCERBUCEr ick'l;\n",
      "e'e IZAsth t we ndofrendere the thofakeon wiche alik n cem, buran aw hice me r gh:\n",
      "Angak beene, p; mind merin s,\n",
      "Toth bll minoutalo de IULLOPSore ERe y p tirthove,\n",
      "CII I'de this ed;\n",
      "Ay tin:\n",
      "\n",
      "APowall o aporere w ts\n",
      "BUKI ff o k,\n",
      "PEEENAs:\n",
      "KI l thed de gbr hion: upsth med pr'dou m ourere,\n",
      "NERI al't wear n, anched, iset shit RAndas towilat t ithlise fad,\n",
      "Wr-d ye ftibeouty;\n",
      "PUThe?\n",
      "Whegenth\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2  # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b, t] = mean_{i<=t} x[b, i]\n",
    "# i.e. each 't'th token will be the average of itself and the tokens behind it. \n",
    "# Note that it is strictly the ones behind it, not the ones in front. We want to predict the future tokens, not already know them\n",
    "\n",
    "# version 1: for loops\n",
    "xbow = torch.zeros((B, T, C))  # bow is short for bag of words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]  # (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)\n",
    "\n",
    "# but this method with for loops is very inefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: matrix multiply\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "\n",
    "xbow2 = wei @ x  \n",
    "# (T, T) @ (B, T, C) ---> pytorch will see these are not shapes are not as we require them, so it will do\n",
    "# (B, T, T) @ (B, T, C) = (B, T, C)  ---> for each batch element, there will be a (T, T) @ (T, C), giving out a (T, C)\n",
    "\n",
    "torch.allclose(xbow, xbow2, atol=1e-6, rtol=1e-4)  # Adjusted tolerance for floating point differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (32) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[245], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m wei \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(wei, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# converts the weights to probabilities, -inf values become zero\u001b[39;00m\n\u001b[0;32m      6\u001b[0m xbow3 \u001b[38;5;241m=\u001b[39m wei \u001b[38;5;241m@\u001b[39m x\n\u001b[1;32m----> 7\u001b[0m torch\u001b[38;5;241m.\u001b[39mallclose(xbow, xbow3, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (32) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))  # will be used as a mask to ensure that each position only attends to previous positions (including itself)\n",
    "wei = torch.zeros((T, T))  \n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # fills the upper triangular part of wei with negative infinity, to ensure that each position cannot attend to future positions\n",
    "wei = F.softmax(wei, dim=-1)  # converts the weights to probabilities, -inf values become zero\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3, atol=1e-6, rtol=1e-4)\n",
    "\n",
    "# NOTE: for the softmax function here, this is what it does:\n",
    "# For the negative infinity values (upper triangle), e^(-inf) = 0\n",
    "# For the zero values (lower triangle), e^0 = 1\n",
    "# then it is transformed into a probability distribution where each row sums to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32  # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# let's see a single head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)  # (B, T, head_size) \n",
    "q = query(x)  # (B, T, head_size)\n",
    "# all tokens in all batches in parallel and independently, produce a key and a query, no communication has happened YET\n",
    "\n",
    "# so what this head does is, each token (ie character in this case), now has two vectors, a key and a query\n",
    "# these will help tokens \"ask questions\" (query) and \"provide information\" (key)\n",
    "# before we used to initialize the weights with all zeros\n",
    "# now we initialize weights in a data dependent manner\n",
    "\n",
    "# so each char will look at what has come before, and which ones are the most important for it\n",
    "# this is done by matrix multiplying it's query with the keys of all tokens that came before and its own key as well\n",
    "# this mat mul will 'align' (by dot product) the weights of that token to the chars that came before\n",
    "# each token's query is dot-producted with every token's key (and it's own key as well)\n",
    "# a high dot product means the query token finds that key token relevant\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5 # we have to be careful with the transpose here for mat mul, we transpose the 2nd last and last dim here\n",
    "# (B, T, 16) @ (B, 16, T) = (B, T, T)\n",
    "# we multiply by head_size**-0.5 to make wei unit variance if the incoming q and k are unit variance too. To prevent saturation of softmax\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros((T, T))  # this could be said to be the 'affinity'. we don't want this to be all uniform ie 0, because different tokens will find different other tokens more or less interesting\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) \n",
    "wei = F.softmax(wei, dim=-1)\n",
    "v = value(x)  # now the x is private to each token. the v is the thing that gets aggretated for the purpose of this single head\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape\n",
    "\n",
    "# Query (q): determines what each token is looking for\n",
    "# Key (k): determines what each token offers\n",
    "# Value (v): represents the actual information that's being passed\n",
    "\n",
    "# The attention weights (wei) determine how important each token is to each other token\n",
    "# we don't necessarily want to pass the raw input (x) directly. Instead, we pass a transformed version (v)\n",
    "# By having separate transformations for key, query, and value, \n",
    "# the model can learn to represent the information in different ways for different purposes\n",
    "\n",
    "# The q @ k.transpose(-2, -1) operation computes a similarity or relevance score\n",
    "# But we might want to pass different information than what we used to compute that score\n",
    "# v allows us to separate \"what determines attention\" from \"what information is passed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]\n",
    "# here in the 8th row, look at the 8th token 0.2391 for example\n",
    "# the 8th token now's what content it has (through character embedding table), and it knows what position it's at (position embedding table)\n",
    "# and now it can send out a 'query', to look through the 'keys' of the previous characters, to know how important each of them is to it\n",
    "# so if a certain character is important to it, those both will have a high affinity\n",
    "# as an example here, the 8th token 0.2391 seems to have a high affinity for the 4th token 0.2297, since they're close in values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- notes and deriving the steps along the way ------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]\n",
    "# the first element remains the same, as there is nothing behind that to average over\n",
    "# the next one is an average of itself and the one behind it\n",
    "# and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "---\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "---\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3,2)).float()  # (3,2) specifies the shape of the tensor\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('---')\n",
    "print(b)\n",
    "print('---')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3, 3))  # gives a lower trianguar matrix i.e. 0 above in the upper half right triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "---\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "---\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0, 10, (3,2)).float()  # (3,2) specifies the shape of the tensor\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('---')\n",
    "print(b)\n",
    "print('---')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we end up just plucking out the 1st row of b to be the 1st row of c\n",
    "# for the 2nd row of c, we end up taking the sums of the 1st and 2nd rows of b\n",
    "# for the 3rd row of c, we take the sums of the 1st, 2nd, and 3rd rows of b\n",
    "\n",
    "# so we have a pretty good way to take the sums we want now (step for mean calculation remains)\n",
    "# and more efficiently instead of having to use for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "---\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "---\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)  # to normalize the values in it, so that we directly get the mean  # all rows will sum to 1 now\n",
    "b = torch.randint(0, 10, (3,2)).float()  # (3,2) specifies the shape of the tensor\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('---')\n",
    "print(b)\n",
    "print('---')\n",
    "print('c=')\n",
    "print(c)\n",
    "\n",
    "# so the c we get here is directly the averaged out tensor we wanted all along\n",
    "# matrix multiplication FTW!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
