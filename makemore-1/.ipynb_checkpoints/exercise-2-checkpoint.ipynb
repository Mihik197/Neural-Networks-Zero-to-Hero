{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('train.txt', 'r').read().splitlines()\n",
    "test_words = open('test.txt', 'r').read().splitlines()\n",
    "dev_words = open('dev.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))   # alphabetically sorted list of unique set of letters (26)\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}  # creates a dict with mapping of index to each letter. eg {'a': 0, 'b':1 ...}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  182539\n"
     ]
    }
   ],
   "source": [
    "# creating the dataset\n",
    "xs, ys = [], []   # inputs, targets\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.462986707687378\n",
      "2.462939500808716\n",
      "2.462892770767212\n",
      "2.4628467559814453\n",
      "2.462801218032837\n",
      "2.4627561569213867\n",
      "2.4627113342285156\n",
      "2.462667226791382\n",
      "2.462623357772827\n",
      "2.4625802040100098\n",
      "2.4625372886657715\n",
      "2.4624950885772705\n",
      "2.4624531269073486\n",
      "2.462411642074585\n",
      "2.4623706340789795\n",
      "2.462329626083374\n",
      "2.462289333343506\n",
      "2.462249517440796\n",
      "2.462209701538086\n",
      "2.4621708393096924\n",
      "2.462131977081299\n",
      "2.4620938301086426\n",
      "2.4620556831359863\n",
      "2.4620180130004883\n",
      "2.4619805812835693\n",
      "2.4619438648223877\n",
      "2.461907148361206\n",
      "2.4618709087371826\n",
      "2.4618351459503174\n",
      "2.4617996215820312\n",
      "2.461764335632324\n",
      "2.4617292881011963\n",
      "2.4616947174072266\n",
      "2.461660623550415\n",
      "2.4616267681121826\n",
      "2.4615931510925293\n",
      "2.461559534072876\n",
      "2.46152663230896\n",
      "2.461493730545044\n",
      "2.4614615440368652\n",
      "2.4614293575286865\n",
      "2.461397409439087\n",
      "2.4613656997680664\n",
      "2.461334705352783\n",
      "2.461303472518921\n",
      "2.4612724781036377\n",
      "2.4612419605255127\n",
      "2.461211919784546\n",
      "2.461181640625\n",
      "2.4611523151397705\n",
      "2.461122751235962\n",
      "2.4610936641693115\n",
      "2.461064577102661\n",
      "2.46103572845459\n",
      "2.4610073566436768\n",
      "2.4609789848327637\n",
      "2.4609508514404297\n",
      "2.460923194885254\n",
      "2.460895538330078\n",
      "2.4608683586120605\n",
      "2.460841178894043\n",
      "2.4608142375946045\n",
      "2.460787534713745\n",
      "2.460761070251465\n",
      "2.4607348442077637\n",
      "2.4607086181640625\n",
      "2.4606826305389404\n",
      "2.4606573581695557\n",
      "2.460631847381592\n",
      "2.460606575012207\n",
      "2.4605817794799805\n",
      "2.460556745529175\n",
      "2.4605319499969482\n",
      "2.460507869720459\n",
      "2.4604833126068115\n",
      "2.4604592323303223\n",
      "2.460435390472412\n",
      "2.460411548614502\n",
      "2.46038818359375\n",
      "2.460364818572998\n",
      "2.460341453552246\n",
      "2.4603185653686523\n",
      "2.4602959156036377\n",
      "2.460273027420044\n",
      "2.4602503776550293\n",
      "2.460228443145752\n",
      "2.4602060317993164\n",
      "2.460184097290039\n",
      "2.4601621627807617\n",
      "2.4601407051086426\n",
      "2.4601190090179443\n",
      "2.4600977897644043\n",
      "2.4600765705108643\n",
      "2.460055351257324\n",
      "2.4600346088409424\n",
      "2.4600136280059814\n",
      "2.459993362426758\n",
      "2.459972620010376\n",
      "2.4599523544311523\n",
      "2.4599320888519287\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "xenc = F.one_hot(xs, num_classes=27).float()  # one hot encoding input to network\n",
    "\n",
    "for k in range(100):\n",
    "\n",
    "    # forward pass\n",
    "    logits = xenc @ W  # predict log-counts\n",
    "    counts = logits.exp()  # equivalent to our initial N matrix containing the original frequencies\n",
    "    probs = counts / counts.sum(1, keepdim=True)  # probabilities for the next character\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() \n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None  # set gradient to zero\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  22729\n"
     ]
    }
   ],
   "source": [
    "# dev set \n",
    "\n",
    "xs, ys = [], []   # inputs, targets\n",
    "\n",
    "for w in test_words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "# W = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "\n",
    "xenc = F.one_hot(xs, num_classes=27).float()  # one hot encoding input to network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4755358695983887\n"
     ]
    }
   ],
   "source": [
    "logits = xenc @ W  # predict log-counts\n",
    "counts = logits.exp()  # equivalent to our initial N matrix containing the original frequencies\n",
    "probs = counts / counts.sum(1, keepdim=True)  # probabilities for the next character\n",
    "loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean()  # second term is called L2 regularization\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  156913\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']   # a single '.' character to indicate start and end of a word\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]  # input\n",
    "        ix2 = stoi[ch2]  # input\n",
    "        ix3 = stoi[ch3]  # output\n",
    "        trigram = (ch1, ch2, ch3)\n",
    "        xs.append([ix1, ix2])  # here we're adding the integer denoting the letter into the array, not the letter itself. because you can't do math on characters ofc\n",
    "        ys.append(ix3)\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.shape[0]\n",
    "print('number of examples: ', num)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*2, 27), generator=g, requires_grad=True)\n",
    "\n",
    "xenc = F.one_hot(xs).float()\n",
    "xenc = xenc.view(-1, 54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.263188123703003\n",
      "2.2628917694091797\n",
      "2.262601375579834\n",
      "2.2623162269592285\n",
      "2.2620365619659424\n",
      "2.2617621421813965\n",
      "2.2614927291870117\n",
      "2.261228322982788\n",
      "2.2609689235687256\n",
      "2.260714054107666\n",
      "2.2604639530181885\n",
      "2.2602181434631348\n",
      "2.259977102279663\n",
      "2.259740114212036\n",
      "2.259506940841675\n",
      "2.2592782974243164\n",
      "2.2590532302856445\n",
      "2.2588322162628174\n",
      "2.2586147785186768\n",
      "2.2584009170532227\n",
      "2.258190870285034\n",
      "2.2579843997955322\n",
      "2.2577810287475586\n",
      "2.2575809955596924\n",
      "2.2573843002319336\n",
      "2.257190465927124\n",
      "2.257000207901001\n",
      "2.256812810897827\n",
      "2.2566277980804443\n",
      "2.256446361541748\n",
      "2.256267547607422\n",
      "2.256091356277466\n",
      "2.255918264389038\n",
      "2.2557473182678223\n",
      "2.2555792331695557\n",
      "2.25541353225708\n",
      "2.2552504539489746\n",
      "2.255089521408081\n",
      "2.2549309730529785\n",
      "2.254775285720825\n",
      "2.2546215057373047\n",
      "2.254469633102417\n",
      "2.2543203830718994\n",
      "2.2541730403900146\n",
      "2.254027843475342\n",
      "2.253884792327881\n",
      "2.2537436485290527\n",
      "2.2536046504974365\n",
      "2.253467082977295\n",
      "2.2533316612243652\n",
      "2.2531981468200684\n",
      "2.2530665397644043\n",
      "2.252936363220215\n",
      "2.2528083324432373\n",
      "2.2526817321777344\n",
      "2.2525570392608643\n",
      "2.2524337768554688\n",
      "2.252312183380127\n",
      "2.2521920204162598\n",
      "2.2520735263824463\n",
      "2.2519567012786865\n",
      "2.2518413066864014\n",
      "2.2517271041870117\n",
      "2.251614809036255\n",
      "2.2515037059783936\n",
      "2.2513937950134277\n",
      "2.2512853145599365\n",
      "2.251178503036499\n",
      "2.251072645187378\n",
      "2.2509679794311523\n",
      "2.2508652210235596\n",
      "2.250763177871704\n",
      "2.250662326812744\n",
      "2.250562906265259\n",
      "2.25046443939209\n",
      "2.2503671646118164\n",
      "2.2502713203430176\n",
      "2.250176191329956\n",
      "2.25008225440979\n",
      "2.2499897480010986\n",
      "2.2498977184295654\n",
      "2.249807119369507\n",
      "2.2497177124023438\n",
      "2.249629020690918\n",
      "2.2495415210723877\n",
      "2.249454975128174\n",
      "2.2493691444396973\n",
      "2.249284505844116\n",
      "2.2492008209228516\n",
      "2.249117851257324\n",
      "2.2490360736846924\n",
      "2.2489547729492188\n",
      "2.2488744258880615\n",
      "2.2487950325012207\n",
      "2.2487165927886963\n",
      "2.248638868331909\n",
      "2.2485620975494385\n",
      "2.248486280441284\n",
      "2.248410940170288\n",
      "2.2483363151550293\n"
     ]
    }
   ],
   "source": [
    "for k in range(100):\n",
    "    # forward pass\n",
    "    logits = xenc @ W  # This will result in shape (196113, 27*27)    \n",
    "    counts = logits.exp()  # equivalent to our initial N matrix containing the original frequencies\n",
    "    probs = counts / counts.sum(1, keepdim=True)  # probabilities for the next character\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None  # set gradient to zero\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  19525\n"
     ]
    }
   ],
   "source": [
    "# dev set \n",
    "\n",
    "xs, ys = [], []   # inputs, targets\n",
    "\n",
    "for w in test_words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]  # input\n",
    "        ix2 = stoi[ch2]  # input\n",
    "        ix3 = stoi[ch3]  # output\n",
    "        trigram = (ch1, ch2, ch3)\n",
    "        xs.append([ix1, ix2])  # here we're adding the integer denoting the letter into the array, not the letter itself. because you can't do math on characters ofc\n",
    "        ys.append(ix3)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.shape[0]\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "xenc = F.one_hot(xs).float()\n",
    "xenc = xenc.view(-1, 54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.246502637863159\n"
     ]
    }
   ],
   "source": [
    "logits = xenc @ W  # predict log-counts\n",
    "counts = logits.exp()  # equivalent to our initial N matrix containing the original frequencies\n",
    "probs = counts / counts.sum(1, keepdim=True)  # probabilities for the next character\n",
    "loss = -probs[torch.arange(num), ys].log().mean() # + 0.1*(W**2).mean()  # second term is called L2 regularization\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  156913\n"
     ]
    }
   ],
   "source": [
    "# lets train the model\n",
    "\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']   # a single '.' character to indicate start and end of a word\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]  # input\n",
    "        ix2 = stoi[ch2]  # input\n",
    "        ix3 = stoi[ch3]  # output\n",
    "        trigram = (ch1, ch2, ch3)\n",
    "        xs.append([ix1, ix2])  # here we're adding the integer denoting the letter into the array, not the letter itself. because you can't do math on characters ofc\n",
    "        ys.append(ix3)\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.shape[0]\n",
    "print('number of examples: ', num)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*2, 27), generator=g, requires_grad=True)\n",
    "\n",
    "xenc = F.one_hot(xs).float()\n",
    "xenc = xenc.view(-1, 54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.244788885116577\n",
      "2.2447590827941895\n",
      "2.244729995727539\n",
      "2.2447006702423096\n",
      "2.2446718215942383\n",
      "2.244642972946167\n",
      "2.244614839553833\n",
      "2.244586706161499\n",
      "2.244558334350586\n",
      "2.24453067779541\n",
      "2.2445027828216553\n",
      "2.2444753646850586\n",
      "2.244448184967041\n",
      "2.2444212436676025\n",
      "2.244394302368164\n",
      "2.244367837905884\n",
      "2.2443411350250244\n",
      "2.2443149089813232\n",
      "2.244288921356201\n",
      "2.244263172149658\n",
      "2.2442374229431152\n",
      "2.2442119121551514\n",
      "2.2441866397857666\n",
      "2.244161605834961\n",
      "2.244136333465576\n",
      "2.2441117763519287\n",
      "2.244086980819702\n",
      "2.244062900543213\n",
      "2.2440385818481445\n",
      "2.244014263153076\n",
      "2.243990659713745\n",
      "2.243966579437256\n",
      "2.243943214416504\n",
      "2.243919849395752\n",
      "2.243896722793579\n",
      "2.2438735961914062\n",
      "2.2438504695892334\n",
      "2.2438278198242188\n",
      "2.243805408477783\n",
      "2.2437827587127686\n",
      "2.243760347366333\n",
      "2.2437379360198975\n",
      "2.243716239929199\n",
      "2.243694543838501\n",
      "2.2436728477478027\n",
      "2.2436511516571045\n",
      "2.2436296939849854\n",
      "2.2436084747314453\n",
      "2.2435874938964844\n",
      "2.2435667514801025\n",
      "2.2435457706451416\n",
      "2.2435250282287598\n",
      "2.243504524230957\n",
      "2.2434842586517334\n",
      "2.2434637546539307\n",
      "2.2434439659118652\n",
      "2.2434237003326416\n",
      "2.243403911590576\n",
      "2.2433841228485107\n",
      "2.2433643341064453\n",
      "2.243345022201538\n",
      "2.24332594871521\n",
      "2.2433066368103027\n",
      "2.2432875633239746\n",
      "2.2432687282562256\n",
      "2.2432498931884766\n",
      "2.2432308197021484\n",
      "2.2432122230529785\n",
      "2.2431938648223877\n",
      "2.243175745010376\n",
      "2.243157386779785\n",
      "2.2431392669677734\n",
      "2.2431211471557617\n",
      "2.243103504180908\n",
      "2.2430856227874756\n",
      "2.243067741394043\n",
      "2.2430503368377686\n",
      "2.243032932281494\n",
      "2.243015766143799\n",
      "2.2429983615875244\n",
      "2.242981195449829\n",
      "2.242964506149292\n",
      "2.2429473400115967\n",
      "2.2429306507110596\n",
      "2.2429141998291016\n",
      "2.2428972721099854\n",
      "2.2428810596466064\n",
      "2.2428646087646484\n",
      "2.2428483963012695\n",
      "2.2428324222564697\n",
      "2.24281644821167\n",
      "2.242799997329712\n",
      "2.2427845001220703\n",
      "2.2427682876586914\n",
      "2.242753028869629\n",
      "2.242737293243408\n",
      "2.2427215576171875\n",
      "2.242706537246704\n",
      "2.2426910400390625\n",
      "2.242676019668579\n"
     ]
    }
   ],
   "source": [
    "for k in range(100):\n",
    "    # forward pass\n",
    "    logits = xenc @ W  # This will result in shape (196113, 27*27)    \n",
    "    counts = logits.exp()  # equivalent to our initial N matrix containing the original frequencies\n",
    "    probs = counts / counts.sum(1, keepdim=True)  # probabilities for the next character\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.001*(W**2).mean() # this smoothens out the weights\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None  # set gradient to zero\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our loss is much higher in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  19525\n"
     ]
    }
   ],
   "source": [
    "# dev set \n",
    "\n",
    "xs, ys = [], []   # inputs, targets\n",
    "\n",
    "for w in test_words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]  # input\n",
    "        ix2 = stoi[ch2]  # input\n",
    "        ix3 = stoi[ch3]  # output\n",
    "        trigram = (ch1, ch2, ch3)\n",
    "        xs.append([ix1, ix2])  # here we're adding the integer denoting the letter into the array, not the letter itself. because you can't do math on characters ofc\n",
    "        ys.append(ix3)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.shape[0]\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "xenc = F.one_hot(xs).float()\n",
    "xenc = xenc.view(-1, 54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.239325761795044\n"
     ]
    }
   ],
   "source": [
    "logits = xenc @ W  # predict log-counts\n",
    "counts = logits.exp()  # equivalent to our initial N matrix containing the original frequencies\n",
    "probs = counts / counts.sum(1, keepdim=True)  # probabilities for the next character\n",
    "loss = -probs[torch.arange(num), ys].log().mean() # + 0.1*(W**2).mean()  # second term is called L2 regularization\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing our use of F.one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  196113\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']   # a single '.' character to indicate start and end of a word\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]  # input\n",
    "        ix2 = stoi[ch2]  # input\n",
    "        ix3 = stoi[ch3]  # output\n",
    "        trigram = (ch1, ch2, ch3)\n",
    "        xs.append([ix1, ix2])  # here we're adding the integer denoting the letter into the array, not the letter itself. because you can't do math on characters ofc\n",
    "        ys.append(ix3)\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.shape[0]\n",
    "print('number of examples: ', num)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*2, 27), generator=g, requires_grad=True)\n",
    "\n",
    "# xenc = F.one_hot(xs).float()\n",
    "# xenc = xenc.view(-1, 54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.7236e-01,  1.4830e+00,  3.1748e-01,  1.0588e+00,  2.3982e+00,\n",
       "         4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2197e-01,  5.1007e-01,\n",
       "         1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01,\n",
       "         5.5570e-01,  4.7458e-01, -1.3867e+00,  1.6229e+00,  1.7197e-01,\n",
       "         9.8846e-01,  5.0657e-01,  1.0198e+00, -1.9062e+00, -4.2753e-01,\n",
       "        -2.1259e+00,  9.6041e-01], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# W[xs[0]].shape\n",
    "W[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.263436794281006\n",
      "2.2631397247314453\n",
      "2.262847900390625\n",
      "2.2625620365142822\n",
      "2.2622814178466797\n",
      "2.2620060443878174\n",
      "2.2617361545562744\n",
      "2.2614707946777344\n",
      "2.2612104415893555\n",
      "2.2609548568725586\n",
      "2.2607038021087646\n",
      "2.2604572772979736\n",
      "2.2602150440216064\n",
      "2.259977102279663\n",
      "2.2597432136535645\n",
      "2.2595133781433105\n",
      "2.2592875957489014\n",
      "2.259065628051758\n",
      "2.25884747505188\n",
      "2.2586326599121094\n",
      "2.2584214210510254\n",
      "2.258213996887207\n",
      "2.258009672164917\n",
      "2.2578089237213135\n",
      "2.2576115131378174\n",
      "2.2574169635772705\n",
      "2.257225751876831\n",
      "2.2570371627807617\n",
      "2.2568519115448\n",
      "2.256669282913208\n",
      "2.2564897537231445\n",
      "2.256312608718872\n",
      "2.256138563156128\n",
      "2.255966901779175\n",
      "2.255798101425171\n",
      "2.255631685256958\n",
      "2.255467653274536\n",
      "2.2553060054779053\n",
      "2.2551469802856445\n",
      "2.2549901008605957\n",
      "2.254835605621338\n",
      "2.254683256149292\n",
      "2.254533052444458\n",
      "2.254384994506836\n",
      "2.2542388439178467\n",
      "2.2540953159332275\n",
      "2.253953456878662\n",
      "2.2538132667541504\n",
      "2.2536752223968506\n",
      "2.2535393238067627\n",
      "2.2534048557281494\n",
      "2.253272771835327\n",
      "2.2531421184539795\n",
      "2.2530131340026855\n",
      "2.2528860569000244\n",
      "2.252760410308838\n",
      "2.252636671066284\n",
      "2.252514600753784\n",
      "2.252393960952759\n",
      "2.252274751663208\n",
      "2.25215744972229\n",
      "2.2520413398742676\n",
      "2.251926898956299\n",
      "2.2518138885498047\n",
      "2.251702070236206\n",
      "2.251591920852661\n",
      "2.2514827251434326\n",
      "2.251375436782837\n",
      "2.2512691020965576\n",
      "2.251163959503174\n",
      "2.2510602474212646\n",
      "2.250958204269409\n",
      "2.250856876373291\n",
      "2.2507569789886475\n",
      "2.2506582736968994\n",
      "2.2505605220794678\n",
      "2.2504642009735107\n",
      "2.25036883354187\n",
      "2.250274658203125\n",
      "2.2501814365386963\n",
      "2.250089406967163\n",
      "2.2499983310699463\n",
      "2.249908447265625\n",
      "2.24981951713562\n",
      "2.2497315406799316\n",
      "2.2496447563171387\n",
      "2.249558448791504\n",
      "2.2494735717773438\n",
      "2.2493896484375\n",
      "2.2493064403533936\n",
      "2.2492239475250244\n",
      "2.24914288520813\n",
      "2.2490622997283936\n",
      "2.2489826679229736\n",
      "2.24890398979187\n",
      "2.248826026916504\n",
      "2.248749017715454\n",
      "2.2486727237701416\n",
      "2.2485973834991455\n",
      "2.2485227584838867\n"
     ]
    }
   ],
   "source": [
    "for k in range(100):\n",
    "    # forward pass\n",
    "    logits = W[xs[:, 0]] + W[xs[:, 1] + 27]  # This helps us make the operation more efficient by skipping over the step of having to one_hot encode\n",
    "    counts = logits.exp()  # equivalent to our initial N matrix containing the original frequencies\n",
    "    probs = counts / counts.sum(1, keepdim=True)  # probabilities for the next character\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None  # set gradient to zero\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  196113\n"
     ]
    }
   ],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']   # a single '.' character to indicate start and end of a word\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]  # input\n",
    "        ix2 = stoi[ch2]  # input\n",
    "        ix3 = stoi[ch3]  # output\n",
    "        trigram = (ch1, ch2, ch3)\n",
    "        xs.append([ix1, ix2])  # here we're adding the integer denoting the letter into the array, not the letter itself. because you can't do math on characters ofc\n",
    "        ys.append(ix3)\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.shape[0]\n",
    "print('number of examples: ', num)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*2, 27), generator=g, requires_grad=True)\n",
    "\n",
    "# xenc = F.one_hot(xs).float()\n",
    "# xenc = xenc.view(-1, 54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.186270713806152\n",
      "3.3573663234710693\n",
      "3.042149543762207\n",
      "2.8714542388916016\n",
      "2.7671947479248047\n",
      "2.694681167602539\n",
      "2.6390926837921143\n",
      "2.5949814319610596\n",
      "2.559002637863159\n",
      "2.5292224884033203\n",
      "2.5042338371276855\n",
      "2.483072519302368\n",
      "2.464961528778076\n",
      "2.4493143558502197\n",
      "2.435654401779175\n",
      "2.423619031906128\n",
      "2.4129199981689453\n",
      "2.4033381938934326\n",
      "2.394700765609741\n",
      "2.386871337890625\n",
      "2.379739999771118\n",
      "2.3732173442840576\n",
      "2.3672289848327637\n",
      "2.3617119789123535\n",
      "2.3566133975982666\n",
      "2.3518881797790527\n",
      "2.34749698638916\n",
      "2.343407154083252\n",
      "2.3395884037017822\n",
      "2.3360161781311035\n",
      "2.332667350769043\n",
      "2.3295230865478516\n",
      "2.3265650272369385\n",
      "2.3237786293029785\n",
      "2.3211495876312256\n",
      "2.3186655044555664\n",
      "2.3163156509399414\n",
      "2.314089059829712\n",
      "2.3119773864746094\n",
      "2.3099722862243652\n",
      "2.3080666065216064\n",
      "2.3062520027160645\n",
      "2.3045237064361572\n",
      "2.3028757572174072\n",
      "2.301301956176758\n",
      "2.2997987270355225\n",
      "2.298360586166382\n",
      "2.2969841957092285\n",
      "2.2956655025482178\n",
      "2.294400691986084\n",
      "2.293187141418457\n",
      "2.292020797729492\n",
      "2.290900230407715\n",
      "2.2898223400115967\n",
      "2.2887840270996094\n",
      "2.2877843379974365\n",
      "2.28682017326355\n",
      "2.2858903408050537\n",
      "2.284992218017578\n",
      "2.2841248512268066\n",
      "2.2832868099212646\n",
      "2.2824761867523193\n",
      "2.281691789627075\n",
      "2.2809317111968994\n",
      "2.280196189880371\n",
      "2.27948260307312\n",
      "2.2787907123565674\n",
      "2.2781195640563965\n",
      "2.277467966079712\n",
      "2.2768354415893555\n",
      "2.2762207984924316\n",
      "2.275623083114624\n",
      "2.2750420570373535\n",
      "2.2744765281677246\n",
      "2.2739264965057373\n",
      "2.273390769958496\n",
      "2.2728688716888428\n",
      "2.2723605632781982\n",
      "2.271865129470825\n",
      "2.2713818550109863\n",
      "2.2709107398986816\n",
      "2.270451307296753\n",
      "2.2700023651123047\n",
      "2.269564390182495\n",
      "2.269136428833008\n",
      "2.268718719482422\n",
      "2.268310308456421\n",
      "2.267910957336426\n",
      "2.2675209045410156\n",
      "2.267138957977295\n",
      "2.26676607131958\n",
      "2.2664005756378174\n",
      "2.2660434246063232\n",
      "2.265693426132202\n",
      "2.265350818634033\n",
      "2.2650153636932373\n",
      "2.2646865844726562\n",
      "2.264364719390869\n",
      "2.2640492916107178\n",
      "2.263740062713623\n"
     ]
    }
   ],
   "source": [
    "for k in range(100):\n",
    "    # forward pass\n",
    "    logits = W[xs[:, 0]] + W[xs[:, 1] + 27]  # This helps us make the operation more efficient by skipping over the step of having to one_hot encode\n",
    "    counts = logits.exp()  # equivalent to our initial N matrix containing the original frequencies\n",
    "    probs = counts / counts.sum(1, keepdim=True)  # probabilities for the next character\n",
    "    \n",
    "    # Calculate loss using cross_entropy()\n",
    "    # older method -> loss = -probs[torch.arange(num), ys].log().mean()\n",
    "    loss = F.cross_entropy(logits, ys)\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None  # set gradient to zero\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    W.data += -50 * W.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
